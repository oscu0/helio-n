{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd2b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import sunpy.map\n",
    "from sunpy.map.maputils import all_coordinates_from_map, coordinate_is_on_solar_disk\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f70e8b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b7af97",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e065bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FITS_ROOT = \"./Data/Training/FITS\"\n",
    "TRAIN_MASKS_ROOT = \"./Data/Training/Masks\"\n",
    "\n",
    "MODEL_PATH = \"model_CH_UNet.h5\"\n",
    "\n",
    "INFER_FITS_ROOT = \"./Data/Inferrence/FITS\"\n",
    "INFER_MASKS_ROOT = \"./Data/Inferrence/Masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c7578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_fits(f):\n",
    "    hpc_coords = all_coordinates_from_map(f)\n",
    "    mask = coordinate_is_on_solar_disk(hpc_coords)\n",
    "    palette = f.cmap.copy()\n",
    "    palette.set_bad(\"black\")\n",
    "    scaled_map = sunpy.map.Map(f.data, f.meta, mask=~mask)\n",
    "    ff = scaled_map.data\n",
    "    return ff\n",
    "\n",
    "\n",
    "def prepare_fits(path, mask_disk=True, clip_low=1, clip_high=99):\n",
    "    f = sunpy.map.Map(path)\n",
    "    if mask_disk:\n",
    "        ff = mask_fits(f).data\n",
    "    else:\n",
    "        ff = f.data\n",
    "\n",
    "    low = np.percentile(ff, clip_low)\n",
    "    high = np.percentile(ff, clip_high)\n",
    "    ff = np.clip(ff, low, high)\n",
    "    ff = (ff - low) / (high - low + 1e-6)\n",
    "    return ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "33ec3737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mask(path):\n",
    "    \"\"\"\n",
    "    Read PNG mask (0/255-ish) and return 0/1 float32 array.\n",
    "\n",
    "    We *don't* rename; we just read your existing _CH_MASK_FINAL.png.\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "\n",
    "    im = Image.open(path).convert(\"L\")\n",
    "    arr = np.array(im, dtype=np.float32)\n",
    "    # Consider >127 as CH\n",
    "    arr = (arr > 127).astype(np.float32)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b4f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(fits_root, masks_root):\n",
    "    def index(p):\n",
    "        return p.split(\"/\")[-1][3:16]\n",
    "\n",
    "    fits_re = re.compile(r\"AIA(\\d{8})_\\d{4,6}_(\\d{4})\\.fits$\")\n",
    "    mask_re = re.compile(r\"AIA(\\d{8})_\\d{6}_(\\d{4})_CH_MASK_FINAL\\.png$\")\n",
    "\n",
    "    # Collect all FITS and masks\n",
    "    fits_files = glob.glob(os.path.join(fits_root, \"**\", \"*.fits\"), recursive=True)\n",
    "    mask_files = glob.glob(\n",
    "        os.path.join(masks_root, \"**\", \"*_CH_MASK_FINAL.png\"), recursive=True\n",
    "    )\n",
    "\n",
    "    df_fits = pd.DataFrame(\n",
    "        {\n",
    "            \"key\": [index(p) for p in fits_files],\n",
    "            \"fits_path\": fits_files,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df_masks = pd.DataFrame(\n",
    "        {\n",
    "            \"key\": [index(p) for p in mask_files],\n",
    "            \"mask_path\": mask_files,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Optional: detect duplicate keys (same timestamp, multiple files)\n",
    "    dup_fits = df_fits[df_fits.duplicated(\"key\", keep=False)]\n",
    "    dup_masks = df_masks[df_masks.duplicated(\"key\", keep=False)]\n",
    "\n",
    "    if not dup_fits.empty:\n",
    "        print(\"⚠ Duplicate keys in FITS:\")\n",
    "        print(dup_fits.sort_values(\"key\"))\n",
    "\n",
    "    if not dup_masks.empty:\n",
    "        print(\"⚠ Duplicate keys in masks:\")\n",
    "        print(dup_masks.sort_values(\"key\"))\n",
    "\n",
    "    # Outer join to see everything in one table\n",
    "    merged = df_fits.merge(df_masks, on=\"key\", how=\"outer\", indicator=True)\n",
    "\n",
    "    matches = merged[merged[\"_merge\"] == \"both\"].copy()\n",
    "    fits_only = merged[merged[\"_merge\"] == \"left_only\"].copy()\n",
    "    masks_only = merged[merged[\"_merge\"] == \"right_only\"].copy()\n",
    "\n",
    "    for df in (matches, fits_only, masks_only):\n",
    "        df.drop(columns=[\"_merge\"], inplace=True)\n",
    "\n",
    "    matches.set_index(matches.key, inplace=True, drop=True)\n",
    "    matches.drop([\"key\"], axis=1, inplace=True)\n",
    "\n",
    "    return matches, fits_only, masks_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3a1a6e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Duplicate keys in FITS:\n",
      "                key                                          fits_path\n",
      "929   20170712_2154  ./Data/Training/FITS/2017/07/12/AIA20170712_21...\n",
      "1018  20170712_2154  ./Data/Training/FITS/2017/07/12/AIA20170712_21...\n",
      "⚠ Duplicate keys in FITS:\n",
      "                key                                          fits_path\n",
      "7128  20170125_2146  ./Data/Inferrence/FITS/2017/01/25/AIA20170125_...\n",
      "7180  20170125_2146  ./Data/Inferrence/FITS/2017/01/25/AIA20170125_...\n"
     ]
    }
   ],
   "source": [
    "train_df = prepare_dataset(TRAIN_FITS_ROOT, TRAIN_MASKS_ROOT)[0]\n",
    "inf_df = prepare_dataset(INFER_FITS_ROOT, INFER_MASKS_ROOT)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9bf673",
   "metadata": {},
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb98bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_pair(img, mask):\n",
    "    # --- random horizontal flip ---\n",
    "    stacked = tf.concat([img, mask], axis=-1)  # (H, W, 2)\n",
    "    stacked = tf.cond(\n",
    "        tf.random.uniform(()) > 0.5,\n",
    "        lambda: tf.image.flip_left_right(stacked),\n",
    "        lambda: stacked,\n",
    "    )\n",
    "    img, mask = stacked[..., :1], stacked[..., 1:]\n",
    "\n",
    "    # --- small random brightness scaling ---\n",
    "\n",
    "    scale = tf.random.uniform((), 0.9, 1.1)  # ±10 %\n",
    "    img = img * scale\n",
    "    img = tf.clip_by_value(img, 0.0, 1.0)\n",
    "\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16896fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_pair_numpy(fits_path, mask_path):\n",
    "    # fits_path, mask_path come from tf.numpy_function as bytes or strings\n",
    "    fits_path = (\n",
    "        fits_path.decode(\"utf-8\")\n",
    "        if isinstance(fits_path, (bytes, np.bytes_))\n",
    "        else fits_path\n",
    "    )\n",
    "    mask_path = (\n",
    "        mask_path.decode(\"utf-8\")\n",
    "        if isinstance(mask_path, (bytes, np.bytes_))\n",
    "        else mask_path\n",
    "    )\n",
    "\n",
    "    img = prepare_fits(fits_path)  # 2D np array\n",
    "    mask = prepare_mask(mask_path)  # 2D np array\n",
    "\n",
    "    img = np.asarray(img, dtype=np.float32)\n",
    "    mask = np.asarray(mask, dtype=np.float32)\n",
    "\n",
    "    if img.ndim == 2:\n",
    "        img = img[..., np.newaxis]\n",
    "    if mask.ndim == 2:\n",
    "        mask = mask[..., np.newaxis]\n",
    "\n",
    "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE), method=\"bilinear\").numpy()\n",
    "    mask = tf.image.resize(mask, (IMG_SIZE, IMG_SIZE), method=\"nearest\").numpy()\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "\n",
    "def load_pair_tf(fits_path, mask_path):\n",
    "    img, mask = tf.numpy_function(\n",
    "        _load_pair_numpy,\n",
    "        [fits_path, mask_path],\n",
    "        [tf.float32, tf.float32],\n",
    "    )\n",
    "\n",
    "    img.set_shape((IMG_SIZE, IMG_SIZE, 1))\n",
    "    mask.set_shape((IMG_SIZE, IMG_SIZE, 1))\n",
    "\n",
    "    img, mask = augment_pair(img, mask)\n",
    "\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd340a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(x, filters):\n",
    "    x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8e94e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(input_shape=(IMG_SIZE, IMG_SIZE, 1), base_filters=32):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = double_conv(inputs, base_filters)\n",
    "    p1 = layers.MaxPool2D(2)(c1)\n",
    "\n",
    "    c2 = double_conv(p1, base_filters * 2)\n",
    "    p2 = layers.MaxPool2D(2)(c2)\n",
    "\n",
    "    c3 = double_conv(p2, base_filters * 4)\n",
    "    p3 = layers.MaxPool2D(2)(c3)\n",
    "\n",
    "    c4 = double_conv(p3, base_filters * 8)\n",
    "    p4 = layers.MaxPool2D(2)(c4)\n",
    "\n",
    "    # Bottleneck\n",
    "    bn = double_conv(p4, base_filters * 16)\n",
    "\n",
    "    # Decoder\n",
    "    u4 = layers.Conv2DTranspose(base_filters * 8, 2, strides=2, padding=\"same\")(bn)\n",
    "    u4 = layers.Concatenate()([u4, c4])\n",
    "    c5 = double_conv(u4, base_filters * 8)\n",
    "\n",
    "    u3 = layers.Conv2DTranspose(base_filters * 4, 2, strides=2, padding=\"same\")(c5)\n",
    "    u3 = layers.Concatenate()([u3, c3])\n",
    "    c6 = double_conv(u3, base_filters * 4)\n",
    "\n",
    "    u2 = layers.Conv2DTranspose(base_filters * 2, 2, strides=2, padding=\"same\")(c6)\n",
    "    u2 = layers.Concatenate()([u2, c2])\n",
    "    c7 = double_conv(u2, base_filters * 2)\n",
    "\n",
    "    u1 = layers.Conv2DTranspose(base_filters, 2, strides=2, padding=\"same\")(c7)\n",
    "    u1 = layers.Concatenate()([u1, c1])\n",
    "    c8 = double_conv(u1, base_filters)\n",
    "\n",
    "    outputs = layers.Conv2D(1, 1, activation=\"sigmoid\")(c8)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name=\"CH_UNet\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af02f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tiny_unet(input_shape=(IMG_SIZE, IMG_SIZE, 1), base_filters=16):\n",
    "    \"\"\"\n",
    "    Smaller, dumber U-Net:\n",
    "      - Only 3 downsampling levels\n",
    "      - Much fewer filters\n",
    "      - Cheaper to train, less capacity\n",
    "\n",
    "    Suitable when you care more about speed than squeezing out last % IoU.\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder (3 levels instead of 4–5)\n",
    "    c1 = double_conv(inputs, base_filters)  #  H x  W\n",
    "    p1 = layers.MaxPool2D(2)(c1)  # H/2 x W/2\n",
    "\n",
    "    c2 = double_conv(p1, base_filters * 2)  # H/2 x W/2\n",
    "    p2 = layers.MaxPool2D(2)(c2)  # H/4 x W/4\n",
    "\n",
    "    c3 = double_conv(p2, base_filters * 4)  # H/4 x W/4\n",
    "    p3 = layers.MaxPool2D(2)(c3)  # H/8 x W/8\n",
    "\n",
    "    # Bottleneck\n",
    "    bn = double_conv(p3, base_filters * 8)\n",
    "\n",
    "    # Decoder\n",
    "    u3 = layers.Conv2DTranspose(base_filters * 4, 2, strides=2, padding=\"same\")(bn)\n",
    "    u3 = layers.Concatenate()([u3, c3])\n",
    "    c4 = double_conv(u3, base_filters * 4)\n",
    "\n",
    "    u2 = layers.Conv2DTranspose(base_filters * 2, 2, strides=2, padding=\"same\")(c4)\n",
    "    u2 = layers.Concatenate()([u2, c2])\n",
    "    c5 = double_conv(u2, base_filters * 2)\n",
    "\n",
    "    u1 = layers.Conv2DTranspose(base_filters, 2, strides=2, padding=\"same\")(c5)\n",
    "    u1 = layers.Concatenate()([u1, c1])\n",
    "    c6 = double_conv(u1, base_filters)\n",
    "\n",
    "    outputs = layers.Conv2D(1, 1, activation=\"sigmoid\")(c6)\n",
    "\n",
    "    model = keras.Model(inputs, outputs, name=\"CH_TinyUNet\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5fc17ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1.0):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    y_true_f = tf.reshape(y_true, [tf.shape(y_true)[0], -1])\n",
    "    y_pred_f = tf.reshape(y_pred, [tf.shape(y_pred)[0], -1])\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=1)\n",
    "    denom = tf.reduce_sum(y_true_f, axis=1) + tf.reduce_sum(y_pred_f, axis=1)\n",
    "\n",
    "    dice = (2.0 * intersection + smooth) / (denom + smooth)\n",
    "    return tf.reduce_mean(dice)\n",
    "\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    bce = keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    return 0.4 * tf.reduce_mean(bce) + 0.6 * (1.0 - dice_coef(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2056220f",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7619f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model():\n",
    "    custom_objects = {\n",
    "        \"bce_dice_loss\": bce_dice_loss,\n",
    "        \"dice_coef\": dice_coef,\n",
    "    }\n",
    "    model = keras.models.load_model(MODEL_PATH, custom_objects=custom_objects)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(pairs_df):\n",
    "    fits_paths = pairs_df[\"fits_path\"].astype(str).tolist()\n",
    "    mask_paths = pairs_df[\"mask_path\"].astype(str).tolist()\n",
    "\n",
    "    if len(fits_paths) == 0:\n",
    "        raise RuntimeError(\"pairs_df is empty: no FITS-mask pairs to train on.\")\n",
    "    if len(fits_paths) != len(mask_paths):\n",
    "        raise RuntimeError(\n",
    "            f\"Mismatch in pairs_df: {len(fits_paths)} FITS vs {len(mask_paths)} masks.\"\n",
    "        )\n",
    "\n",
    "    n_total = len(fits_paths)\n",
    "    print(f\"Training on {n_total} FITS-mask pairs\")\n",
    "\n",
    "    fits_paths_tf = tf.constant(fits_paths)\n",
    "    mask_paths_tf = tf.constant(mask_paths)\n",
    "\n",
    "    # 90/10 split\n",
    "    n_val = max(1, int(0.1 * n_total))\n",
    "    n_train = n_total - n_val\n",
    "\n",
    "    train_fits = fits_paths_tf[:n_train]\n",
    "    train_masks = mask_paths_tf[:n_train]\n",
    "    val_fits = fits_paths_tf[n_train:]\n",
    "    val_masks = mask_paths_tf[n_train:]\n",
    "\n",
    "    # steps per epoch (integer)\n",
    "    steps_per_epoch = n_train // BATCH_SIZE\n",
    "    val_steps = max(1, n_val // BATCH_SIZE)\n",
    "\n",
    "    # --- datasets ---\n",
    "\n",
    "    train_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((train_fits, train_masks))\n",
    "        .shuffle(buffer_size=n_train)\n",
    "        .map(\n",
    "            lambda f, m: load_pair_tf(f, m),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        )\n",
    "        .repeat()  # important: infinite stream\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    val_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((val_fits, val_masks))\n",
    "        .map(\n",
    "            lambda f, m: load_pair_tf(f, m),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        )\n",
    "        .repeat()  # also infinite so Keras can do val_steps each epoch\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    # --- model ---\n",
    "\n",
    "    model = build_unet(base_filters=32)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=bce_dice_loss,\n",
    "        metrics=[dice_coef, \"accuracy\"],\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            MODEL_PATH,\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            verbose=1,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_ds,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    print(f\"Training finished. Best model saved to {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4652d9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 2364 FITS-mask pairs\n",
      "Epoch 1/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630ms/step - accuracy: 0.8440 - dice_coef: 0.0789 - loss: 0.7294"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 671ms/step - accuracy: 0.9347 - dice_coef: 0.1098 - loss: 0.6641 - val_accuracy: 0.9261 - val_dice_coef: 0.1521 - val_loss: 0.6352 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603ms/step - accuracy: 0.9760 - dice_coef: 0.1976 - loss: 0.5562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 642ms/step - accuracy: 0.9794 - dice_coef: 0.2308 - loss: 0.5263 - val_accuracy: 0.9662 - val_dice_coef: 0.1839 - val_loss: 0.5540 - learning_rate: 1.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 642ms/step - accuracy: 0.9870 - dice_coef: 0.3790 - loss: 0.4087 - val_accuracy: 0.9672 - val_dice_coef: 0.1436 - val_loss: 0.5687 - learning_rate: 1.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614ms/step - accuracy: 0.9899 - dice_coef: 0.5011 - loss: 0.3235"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 652ms/step - accuracy: 0.9904 - dice_coef: 0.5436 - loss: 0.2956 - val_accuracy: 0.9695 - val_dice_coef: 0.2684 - val_loss: 0.4924 - learning_rate: 1.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 683ms/step - accuracy: 0.9923 - dice_coef: 0.6794 - loss: 0.2076 - val_accuracy: 0.9683 - val_dice_coef: 0.2478 - val_loss: 0.5121 - learning_rate: 1.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621ms/step - accuracy: 0.9931 - dice_coef: 0.7447 - loss: 0.1661"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 661ms/step - accuracy: 0.9932 - dice_coef: 0.7597 - loss: 0.1568 - val_accuracy: 0.9680 - val_dice_coef: 0.3071 - val_loss: 0.4822 - learning_rate: 1.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 653ms/step - accuracy: 0.9938 - dice_coef: 0.8083 - loss: 0.1262 - val_accuracy: 0.9686 - val_dice_coef: 0.2994 - val_loss: 0.4915 - learning_rate: 1.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 644ms/step - accuracy: 0.9942 - dice_coef: 0.8369 - loss: 0.1082 - val_accuracy: 0.9693 - val_dice_coef: 0.3126 - val_loss: 0.4867 - learning_rate: 1.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605ms/step - accuracy: 0.9945 - dice_coef: 0.8521 - loss: 0.0987"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 643ms/step - accuracy: 0.9946 - dice_coef: 0.8560 - loss: 0.0962 - val_accuracy: 0.9692 - val_dice_coef: 0.3932 - val_loss: 0.4402 - learning_rate: 1.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 659ms/step - accuracy: 0.9948 - dice_coef: 0.8679 - loss: 0.0886 - val_accuracy: 0.9693 - val_dice_coef: 0.3551 - val_loss: 0.4674 - learning_rate: 1.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 658ms/step - accuracy: 0.9949 - dice_coef: 0.8741 - loss: 0.0847 - val_accuracy: 0.9672 - val_dice_coef: 0.2490 - val_loss: 0.5408 - learning_rate: 1.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 621ms/step - accuracy: 0.9952 - dice_coef: 0.8813 - loss: 0.0800\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 658ms/step - accuracy: 0.9951 - dice_coef: 0.8821 - loss: 0.0796 - val_accuracy: 0.9692 - val_dice_coef: 0.3062 - val_loss: 0.5035 - learning_rate: 1.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m351s\u001b[0m 658ms/step - accuracy: 0.9956 - dice_coef: 0.8946 - loss: 0.0712 - val_accuracy: 0.9698 - val_dice_coef: 0.3622 - val_loss: 0.4684 - learning_rate: 5.0000e-05\n",
      "Epoch 14/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 641ms/step - accuracy: 0.9958 - dice_coef: 0.8997 - loss: 0.0679 - val_accuracy: 0.9686 - val_dice_coef: 0.3317 - val_loss: 0.4910 - learning_rate: 5.0000e-05\n",
      "Epoch 15/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615ms/step - accuracy: 0.9960 - dice_coef: 0.9042 - loss: 0.0648\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 652ms/step - accuracy: 0.9959 - dice_coef: 0.9035 - loss: 0.0654 - val_accuracy: 0.9698 - val_dice_coef: 0.3690 - val_loss: 0.4669 - learning_rate: 5.0000e-05\n",
      "Epoch 16/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 638ms/step - accuracy: 0.9962 - dice_coef: 0.9087 - loss: 0.0619 - val_accuracy: 0.9693 - val_dice_coef: 0.3441 - val_loss: 0.4846 - learning_rate: 2.5000e-05\n",
      "Epoch 17/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 645ms/step - accuracy: 0.9963 - dice_coef: 0.9115 - loss: 0.0601 - val_accuracy: 0.9698 - val_dice_coef: 0.3559 - val_loss: 0.4757 - learning_rate: 2.5000e-05\n",
      "Epoch 18/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614ms/step - accuracy: 0.9963 - dice_coef: 0.9151 - loss: 0.0578\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 651ms/step - accuracy: 0.9963 - dice_coef: 0.9136 - loss: 0.0587 - val_accuracy: 0.9685 - val_dice_coef: 0.2954 - val_loss: 0.5190 - learning_rate: 2.5000e-05\n",
      "Epoch 19/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 656ms/step - accuracy: 0.9964 - dice_coef: 0.9161 - loss: 0.0571 - val_accuracy: 0.9694 - val_dice_coef: 0.3489 - val_loss: 0.4828 - learning_rate: 1.2500e-05\n",
      "Epoch 20/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 662ms/step - accuracy: 0.9965 - dice_coef: 0.9180 - loss: 0.0558 - val_accuracy: 0.9699 - val_dice_coef: 0.3671 - val_loss: 0.4718 - learning_rate: 1.2500e-05\n",
      "Epoch 21/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591ms/step - accuracy: 0.9966 - dice_coef: 0.9205 - loss: 0.0542\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 627ms/step - accuracy: 0.9965 - dice_coef: 0.9190 - loss: 0.0551 - val_accuracy: 0.9696 - val_dice_coef: 0.3709 - val_loss: 0.4712 - learning_rate: 1.2500e-05\n",
      "Epoch 22/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 616ms/step - accuracy: 0.9966 - dice_coef: 0.9207 - loss: 0.0540 - val_accuracy: 0.9695 - val_dice_coef: 0.3554 - val_loss: 0.4814 - learning_rate: 6.2500e-06\n",
      "Epoch 23/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 611ms/step - accuracy: 0.9966 - dice_coef: 0.9215 - loss: 0.0534 - val_accuracy: 0.9695 - val_dice_coef: 0.3520 - val_loss: 0.4845 - learning_rate: 6.2500e-06\n",
      "Epoch 24/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575ms/step - accuracy: 0.9966 - dice_coef: 0.9215 - loss: 0.0535\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 611ms/step - accuracy: 0.9967 - dice_coef: 0.9221 - loss: 0.0531 - val_accuracy: 0.9696 - val_dice_coef: 0.3552 - val_loss: 0.4823 - learning_rate: 6.2500e-06\n",
      "Epoch 25/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 614ms/step - accuracy: 0.9967 - dice_coef: 0.9229 - loss: 0.0526 - val_accuracy: 0.9696 - val_dice_coef: 0.3573 - val_loss: 0.4808 - learning_rate: 3.1250e-06\n",
      "Epoch 26/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 611ms/step - accuracy: 0.9967 - dice_coef: 0.9234 - loss: 0.0522 - val_accuracy: 0.9695 - val_dice_coef: 0.3519 - val_loss: 0.4846 - learning_rate: 3.1250e-06\n",
      "Epoch 27/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 576ms/step - accuracy: 0.9968 - dice_coef: 0.9246 - loss: 0.0513\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 612ms/step - accuracy: 0.9967 - dice_coef: 0.9238 - loss: 0.0520 - val_accuracy: 0.9696 - val_dice_coef: 0.3570 - val_loss: 0.4817 - learning_rate: 3.1250e-06\n",
      "Epoch 28/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 609ms/step - accuracy: 0.9967 - dice_coef: 0.9242 - loss: 0.0517 - val_accuracy: 0.9696 - val_dice_coef: 0.3536 - val_loss: 0.4842 - learning_rate: 1.5625e-06\n",
      "Epoch 29/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 611ms/step - accuracy: 0.9968 - dice_coef: 0.9244 - loss: 0.0516 - val_accuracy: 0.9697 - val_dice_coef: 0.3625 - val_loss: 0.4785 - learning_rate: 1.5625e-06\n",
      "Epoch 30/30\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578ms/step - accuracy: 0.9968 - dice_coef: 0.9258 - loss: 0.0508\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 614ms/step - accuracy: 0.9968 - dice_coef: 0.9245 - loss: 0.0515 - val_accuracy: 0.9694 - val_dice_coef: 0.3517 - val_loss: 0.4860 - learning_rate: 1.5625e-06\n",
      "Training finished. Best model saved to model_CH_UNet.h5\n"
     ]
    }
   ],
   "source": [
    "train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a6e922ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = load_trained_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939a0a62",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_to_array(model, img2d, resize=False):\n",
    "    \"\"\"\n",
    "    img2d: 2D numpy array (H, W), already preprocessed (normalized, disk masked, etc.)\n",
    "    model: your trained Keras model\n",
    "\n",
    "    Returns:\n",
    "        prob_map: 2D numpy array (img_size, img_size) with values in [0, 1]\n",
    "    \"\"\"\n",
    "    img = np.asarray(img2d, dtype=np.float32)\n",
    "\n",
    "    if img.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D array, got shape {img.shape}\")\n",
    "\n",
    "    # Optional: resize if shape doesn't match training size\n",
    "    if resize and img.shape != (IMG_SIZE, IMG_SIZE):\n",
    "        import tensorflow as tf\n",
    "\n",
    "        img_tf = tf.convert_to_tensor(img[..., np.newaxis])  # (H, W, 1)\n",
    "        img_tf = tf.image.resize(img_tf, (IMG_SIZE, IMG_SIZE), method=\"bilinear\")\n",
    "        img = img_tf.numpy()[..., 0]\n",
    "\n",
    "    # Add channel and batch dims: (H, W) → (1, H, W, 1)\n",
    "    x = img[np.newaxis, ..., np.newaxis]  # (1, H, W, 1)\n",
    "\n",
    "    # Predict\n",
    "    prob = model.predict(x, verbose=0)[0, ..., 0]  # back to (H, W)\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdc42d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_via_model(path):\n",
    "    prob_map = apply_model_to_array(model, path, resize=True)\n",
    "    mask = (prob_map > 0.5).astype(np.float32)  # binary mask if you want it\n",
    "    mask_uint8 = (mask * 255).clip(0, 255).astype(np.uint8)\n",
    "    img = PIL.Image.fromarray(mask_uint8, mode=\"L\")\n",
    "\n",
    "    if img.size != (1024, 1024):\n",
    "        img = img.resize((1024, 1024), resample=PIL.Image.NEAREST)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f863417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_map_via_model(path):\n",
    "    prob_map = apply_model_to_array(model, path, resize=True)\n",
    "    img_uint8 = np.clip(prob_map * 255, 0, 255).astype(np.uint8)\n",
    "    img = PIL.Image.fromarray(img_uint8, mode=\"L\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3e033410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mask(row, sdo=False):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(np.array(mask_via_model(prepare_fits(row.fits_path))), cmap=\"gray\")\n",
    "    # plt.title(titles[0])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(np.array(PIL.Image.open(row.mask_path)), cmap=\"gray\")\n",
    "    # plt.title(titles[1])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_sdo(row):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(prepare_fits(row.fits_path), cmap=\"gray\")\n",
    "    plt.contour(np.array(mask_via_model(prepare_fits(row.fits_path))), levels=[0.5], colors=\"red\", linewidths=1.5)\n",
    "    # plt.title(titles[0])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(prepare_fits(row.fits_path), cmap=\"gray\")\n",
    "    plt.contour(np.array(prepare_mask(row.mask_path)), levels=[0.5], colors=\"red\", linewidths=1.5)\n",
    "    # plt.title(titles[1])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8069f120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab86197b58ba484a859c68486d61ba89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(RadioButtons(description='DataFrame:', options=('train', 'inferrence'), value='train'), IntSlid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# 1. Register your dataframes here\n",
    "dfs = {\n",
    "    \"train\": train_df,\n",
    "    \"inferrence\": inf_df,\n",
    "}\n",
    "\n",
    "# 2. Widgets\n",
    "df_selector = widgets.RadioButtons(\n",
    "    options=list(dfs.keys()),\n",
    "    value=\"train\",\n",
    "    description=\"DataFrame:\",\n",
    ")\n",
    "\n",
    "idx_slider = widgets.IntSlider(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len(dfs[\"train\"]) - 1,\n",
    "    step=1,\n",
    "    description=\"Index:\",\n",
    "    continuous_update=False,\n",
    ")\n",
    "\n",
    "show_mask_checkbox = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Show mask (off = SDO)\",\n",
    ")\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "# 3. Update slider range when DF changes\n",
    "def update_slider_range(change):\n",
    "    df = dfs[df_selector.value]\n",
    "    idx_slider.max = max(0, len(df) - 1)\n",
    "    if idx_slider.value > idx_slider.max:\n",
    "        idx_slider.value = idx_slider.max\n",
    "\n",
    "df_selector.observe(update_slider_range, names=\"value\")\n",
    "\n",
    "# 4. Main update function\n",
    "def update_plot(change=None):\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        df = dfs[df_selector.value]\n",
    "        if len(df) == 0:\n",
    "            print(\"Selected dataframe is empty.\")\n",
    "            return\n",
    "\n",
    "        row = df.iloc[idx_slider.value]\n",
    "\n",
    "        if show_mask_checkbox.value:\n",
    "            # You implement this: should plot SDO + NN/IDL mask\n",
    "            plot_mask(row)\n",
    "        else:\n",
    "            # You implement this: should plot just the SDO / FITS-based view\n",
    "            plot_sdo(row)\n",
    "\n",
    "# 5. Hook up callbacks\n",
    "idx_slider.observe(update_plot, names=\"value\")\n",
    "show_mask_checkbox.observe(update_plot, names=\"value\")\n",
    "df_selector.observe(update_plot, names=\"value\")\n",
    "\n",
    "# 6. Display the UI\n",
    "ui = widgets.VBox([\n",
    "    df_selector,\n",
    "    idx_slider,\n",
    "    show_mask_checkbox,\n",
    "    out,\n",
    "])\n",
    "\n",
    "display(ui)\n",
    "\n",
    "# Initial draw\n",
    "update_slider_range(None)\n",
    "update_plot(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db8a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icme3.10 (TF, Metal)",
   "language": "python",
   "name": "icme3.10-nn-metal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
